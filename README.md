The project is located in the Jupyter Notebook Titled NHL_Prediction. The code is divided into sections and sub-sections. Explanations are provided above every 2~3 cells to inform the reader of the intention of each block of code. The three main sections (with titles starting with either 1, 2, or 3) cover importing and cleaning data (section 1), creating a classification model (section 2) and implementing the model for prediction (section 3).

The cells in the notebook are to be excecuted in the order in which they occur. If any errors arise, return to section 1.5 (where the code begins), restart the kernel, and excecute the cells again (in the order they occur). For further reader comprehension, dostrings and comments are provided in the cells, and can be read if the reader desires a more in-depth understanding of what the code is intended to do.

The first cell in section 2.3 can be skipped. running this cell can take over 30 minutes, and is not necessary for the rest of the code to run. It is also important to note that the first cell in section 2.3.2 can also be skipped, as it also takes an excessive amount of time to run. It is not necessary to skip either of these cells, but if the reader has tight time constraints, this is a good choice.

Finally, sections 1.3 and 1.4 can be ignored, and is only intended to be read if one wants to gain a further understanding of the statistics used in the creation of the models in this project.

We all worked on figuring out the project direction and finding data.
Jack Carscadden - Cleaned data, implemented and optimized SVM, wrote report.
Tanmay Patel - Analyzed SVM and PCA, made presentation, made figures.
Adam Dockery - Cleaned data, implemented random forest, made model predictions, made figures.
Mitchell Davis - did not participate.